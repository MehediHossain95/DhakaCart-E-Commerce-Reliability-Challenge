# Architecture & Design Documentation

## ğŸ—ï¸ System Architecture

### High-Level Overview

```
Internet Users (100,000+)
        â†“
   AWS Internet Gateway
        â†“
Application Load Balancer (ALB)
        â†“
Kubernetes Ingress Controller
        â†“
Frontend Service (LoadBalancer)  â†â†’  Backend Service (ClusterIP)
    (3-8 pods)                           (3-10 pods)
        â†“
  Nginx Reverse Proxy
  HTML Dashboard UI
        â†“
    API Calls
        â†“
  Node.js Express API
  - Health Checks
  - Request Logging
  - Error Handling
        â†“
    AWS RDS Database
    - Multi-AZ Failover
    - Encrypted at Rest
    - Daily Snapshots
```

---

## ğŸŒ Network Architecture

### VPC Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AWS VPC (10.0.0.0/16)                                       â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Public Subnet (10.0.1.0/24)                         â”‚   â”‚
â”‚  â”‚  - Internet Gateway                                   â”‚   â”‚
â”‚  â”‚  - NAT Gateway                                        â”‚   â”‚
â”‚  â”‚  - Application Load Balancer                          â”‚   â”‚
â”‚  â”‚  - Kubernetes Ingress Controller                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â†“                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Private Subnet (10.0.2.0/24)                        â”‚   â”‚
â”‚  â”‚  - Kubernetes Nodes                                   â”‚   â”‚
â”‚  â”‚  - Frontend & Backend Pods                            â”‚   â”‚
â”‚  â”‚  - Monitoring Stack                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â†“                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Database Subnet (10.0.3.0/24)                       â”‚   â”‚
â”‚  â”‚  - AWS RDS (Private)                                 â”‚   â”‚
â”‚  â”‚  - No internet access (high security)                â”‚   â”‚
â”‚  â”‚  - Multi-AZ replication                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Security Groups

#### Internet Gateway Security Group
- **Inbound:** HTTP (80), HTTPS (443) from 0.0.0.0/0
- **Outbound:** All traffic allowed

#### Kubernetes Node Security Group
- **Inbound:** 
  - SSH (22) from Admin IPs only
  - K3s API (6443) from VPC only
  - Pod networking (ephemeral ports)
- **Outbound:** All traffic allowed

#### Database Security Group
- **Inbound:** PostgreSQL (5432) from K8s nodes only
- **Outbound:** None (isolated)

---

## ğŸ³ Container Architecture

### Docker Image Hierarchy

```
Base Images (Official)
â”œâ”€ node:18-alpine
â”‚  â””â”€ Backend Dockerfile
â”‚     â”œâ”€ Dependencies layer (npm install)
â”‚     â”œâ”€ Application layer
â”‚     â”œâ”€ Security context (non-root)
â”‚     â””â”€ Health check endpoint
â”‚
â””â”€ nginx:alpine (Multi-stage)
   â”œâ”€ Builder stage
   â”‚  â”œâ”€ node:18-alpine
   â”‚  â”œâ”€ Build frontend
   â”‚  â””â”€ Optimize assets
   â”‚
   â””â”€ Runtime stage
      â”œâ”€ nginx:alpine
      â”œâ”€ Copy built assets
      â”œâ”€ Configuration
      â””â”€ Start nginx daemon
```

### Container Security

**Backend Container:**
```dockerfile
USER node              # Non-root user
EXPOSE 5000
HEALTHCHECK CMD curl -f http://localhost:5000/health

# Security context in K8s:
runAsNonRoot: true
runAsUser: 1000
readOnlyRootFilesystem: true
```

**Frontend Container:**
```dockerfile
USER nginx            # nginx:101 (Alpine)
EXPOSE 80
```

---

## â˜¸ï¸ Kubernetes Architecture

### Pod Topology

```
Kubernetes Cluster
â”œâ”€ Namespace: default
â”‚
â”œâ”€ Deployment: dhakacart-backend
â”‚  â”œâ”€ Replicas: 3 (min) â†’ 10 (max)
â”‚  â”œâ”€ Strategy: RollingUpdate (maxSurge=1, maxUnavailable=0)
â”‚  â”œâ”€ Pod Affinity: Anti-affinity (spread across nodes)
â”‚  â””â”€ Each Pod:
â”‚     â”œâ”€ Container: dhakacart-backend
â”‚     â”œâ”€ Resources:
â”‚     â”‚  â”œâ”€ Requests: 100m CPU, 128Mi Memory
â”‚     â”‚  â””â”€ Limits: 500m CPU, 512Mi Memory
â”‚     â”œâ”€ Probes:
â”‚     â”‚  â”œâ”€ Liveness: /health every 10s
â”‚     â”‚  â””â”€ Readiness: /ready every 5s
â”‚     â””â”€ Security: runAsNonRoot=true
â”‚
â”œâ”€ Deployment: dhakacart-frontend
â”‚  â”œâ”€ Replicas: 3 (min) â†’ 8 (max)
â”‚  â”œâ”€ Strategy: RollingUpdate
â”‚  â”œâ”€ Pod Affinity: Anti-affinity
â”‚  â””â”€ Each Pod:
â”‚     â”œâ”€ Container: dhakacart-frontend
â”‚     â”œâ”€ Resources:
â”‚     â”‚  â”œâ”€ Requests: 50m CPU, 64Mi Memory
â”‚     â”‚  â””â”€ Limits: 250m CPU, 256Mi Memory
â”‚     â””â”€ Environment: API_URL via Service DNS
â”‚
â”œâ”€ Service: dhakacart-backend-service
â”‚  â”œâ”€ Type: ClusterIP (internal only)
â”‚  â”œâ”€ Port: 5000
â”‚  â””â”€ Selector: app=backend
â”‚
â”œâ”€ Service: dhakacart-frontend-service
â”‚  â”œâ”€ Type: ClusterIP
â”‚  â”œâ”€ Port: 80
â”‚  â””â”€ Selector: app=frontend
â”‚
â”œâ”€ Ingress: dhakacart-ingress
â”‚  â”œâ”€ Class: nginx
â”‚  â”œâ”€ Rule: / â†’ frontend:80
â”‚  â”œâ”€ Rule: /api â†’ backend:5000
â”‚  â””â”€ TLS: Let's Encrypt (future)
â”‚
â”œâ”€ HPA: backend-hpa
â”‚  â”œâ”€ Min replicas: 3
â”‚  â”œâ”€ Max replicas: 10
â”‚  â”œâ”€ CPU target: 70%
â”‚  â””â”€ Memory target: 80%
â”‚
â”œâ”€ HPA: frontend-hpa
â”‚  â”œâ”€ Min replicas: 3
â”‚  â”œâ”€ Max replicas: 8
â”‚  â”œâ”€ CPU target: 75%
â”‚  â””â”€ Memory target: 85%
â”‚
â”œâ”€ NetworkPolicy: dhakacart-network-policy
â”‚  â”œâ”€ Frontend â† Ingress Controller
â”‚  â”œâ”€ Backend â† Frontend
â”‚  â””â”€ Database â† Backend (future)
â”‚
â””â”€ Deployment: prometheus (Monitoring)
   â”œâ”€ ServiceAccount: prometheus
   â”œâ”€ ClusterRole: scrape all pods
   â””â”€ PVC: metrics storage
```

### Deployment Process

```
1. Developer pushes code to GitHub
              â†“
2. GitHub Actions triggered
   â”œâ”€ Run tests (backend/frontend)
   â”œâ”€ Security scan (Trivy)
   â””â”€ Build Docker images
              â†“
3. Push images to GHCR
              â†“
4. Deploy to Kubernetes
   â”œâ”€ kubectl apply -f k8s/
   â”œâ”€ Rolling update starts
   â”‚  â”œâ”€ Surge pod created (4 total)
   â”‚  â”œâ”€ New pod passes readiness
   â”‚  â”œâ”€ Old pod drained gracefully
   â”‚  â””â”€ Process repeats until all updated
   â””â”€ Health checks verify stability
              â†“
5. Send Slack notification
   â”œâ”€ âœ… Deployment successful
   â””â”€ Version: SHA-XXXXX
```

---

## ğŸ’¾ Database Architecture

### RDS Setup

```
AWS RDS Instance (Multi-AZ)
â”œâ”€ Engine: PostgreSQL 14
â”œâ”€ Instance Class: db.t3.micro (development)
â”œâ”€ Storage: 20GB gp2
â”œâ”€ Multi-AZ: Enabled
â”‚  â”œâ”€ Primary (ap-southeast-1a)
â”‚  â””â”€ Standby (ap-southeast-1b)
â”œâ”€ Backup:
â”‚  â”œâ”€ Backup retention: 7 days
â”‚  â”œâ”€ Automated snapshots daily
â”‚  â””â”€ S3 backup storage
â”œâ”€ Security:
â”‚  â”œâ”€ Encryption at rest (AWS KMS)
â”‚  â”œâ”€ Encryption in transit (SSL/TLS)
â”‚  â”œâ”€ VPC endpoint (private)
â”‚  â”œâ”€ Security group (Backend pods only)
â”‚  â””â”€ Secrets Manager (password)
â””â”€ Monitoring:
   â”œâ”€ CloudWatch metrics
   â”œâ”€ Enhanced monitoring
   â””â”€ Alerts on:
      â”œâ”€ CPU > 80%
      â”œâ”€ Storage > 80%
      â””â”€ Connection errors
```

### Database Schema (Future)

```sql
-- Products table
CREATE TABLE products (
  id SERIAL PRIMARY KEY,
  name VARCHAR(255) NOT NULL,
  description TEXT,
  price DECIMAL(10, 2),
  stock INT DEFAULT 0,
  category VARCHAR(100),
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);

-- Orders table
CREATE TABLE orders (
  id SERIAL PRIMARY KEY,
  order_number VARCHAR(50) UNIQUE,
  customer_id INT,
  total_amount DECIMAL(10, 2),
  status VARCHAR(50),
  created_at TIMESTAMP DEFAULT NOW()
);

-- Create indices for performance
CREATE INDEX idx_products_category ON products(category);
CREATE INDEX idx_orders_customer ON orders(customer_id);
CREATE INDEX idx_orders_created ON orders(created_at);
```

---

## ğŸ“Š Monitoring & Observability

### Metrics Collection

```
Prometheus Scraping
â”œâ”€ Kubernetes Metrics Server
â”‚  â”œâ”€ CPU usage per pod
â”‚  â”œâ”€ Memory usage per pod
â”‚  â”œâ”€ Network I/O
â”‚  â””â”€ Storage usage
â”‚
â”œâ”€ Backend Pods (/metrics)
â”‚  â”œâ”€ HTTP request rate
â”‚  â”œâ”€ Response time (p50, p95, p99)
â”‚  â”œâ”€ Error rate (4xx, 5xx)
â”‚  â””â”€ Business metrics (items sold, etc.)
â”‚
â””â”€ Custom Alerts
   â”œâ”€ CPU > 80%
   â”œâ”€ Memory > 85%
   â”œâ”€ Response time > 1s
   â”œâ”€ Error rate > 1%
   â””â”€ Pod restarts > 3
```

### Grafana Dashboards

1. **System Health Dashboard**
   - Node CPU/Memory/Disk
   - Pod resource usage
   - Network I/O

2. **Application Dashboard**
   - Request rate (req/sec)
   - Response latency (ms)
   - Error rate (%)
   - Top endpoints

3. **Scaling Dashboard**
   - Current replicas
   - Desired replicas
   - CPU/Memory trends
   - Scaling events log

---

## ğŸ” Security Architecture

### Defense in Depth

```
Layer 1: Network Level
â”œâ”€ AWS Security Groups (firewall rules)
â”œâ”€ Network ACLs
â”œâ”€ VPC isolation (public/private subnets)
â””â”€ Private database subnet

Layer 2: Kubernetes Level
â”œâ”€ Network Policies (pod-to-pod networking)
â”œâ”€ Pod Security Standards
â”œâ”€ RBAC (role-based access control)
â””â”€ Service Account restrictions

Layer 3: Application Level
â”œâ”€ HTTPS/TLS encryption
â”œâ”€ Input validation
â”œâ”€ Rate limiting
â”œâ”€ SQL prepared statements
â””â”€ Error handling (no sensitive data in errors)

Layer 4: Data Level
â”œâ”€ Database encryption at rest (KMS)
â”œâ”€ Encryption in transit (SSL/TLS)
â”œâ”€ Secrets in AWS Secrets Manager
â””â”€ Audit logging
```

### Secrets Management

```
Credentials Flow
â”œâ”€ AWS Secrets Manager
â”‚  â”œâ”€ Database password
â”‚  â”œâ”€ API keys
â”‚  â””â”€ JWT secrets
â”‚
â”œâ”€ Kubernetes Secrets
â”‚  â”œâ”€ Base64 encoded (for pod mount)
â”‚  â”œâ”€ Mounted as volumes
â”‚  â””â”€ Never in logs
â”‚
â””â”€ Pod Environment Variables
   â””â”€ Set from secrets
```

---

## ğŸ”„ CI/CD Architecture

### GitHub Actions Workflow

```
Event: git push
â”œâ”€ Trigger: main or feature branches
â”œâ”€ Concurrency: max 1 deployment per branch
â”‚
â”œâ”€ Job 1: Test Backend
â”‚  â”œâ”€ Checkout code
â”‚  â”œâ”€ Setup Node.js 18
â”‚  â”œâ”€ npm install
â”‚  â”œâ”€ npm lint
â”‚  â””â”€ npm test
â”‚
â”œâ”€ Job 2: Test Frontend
â”‚  â”œâ”€ Checkout code
â”‚  â”œâ”€ Setup Node.js 18
â”‚  â”œâ”€ npm install
â”‚  â”œâ”€ npm build
â”‚  â””â”€ npm test
â”‚
â”œâ”€ Job 3: Security Scan
â”‚  â”œâ”€ Checkout code
â”‚  â”œâ”€ Run Trivy scan
â”‚  â””â”€ Upload SARIF results
â”‚
â”œâ”€ Job 4: Build & Push (requires tests to pass)
â”‚  â”œâ”€ Setup buildx
â”‚  â”œâ”€ Login to GHCR
â”‚  â”œâ”€ Build backend image
â”‚  â”œâ”€ Push backend image
â”‚  â”œâ”€ Build frontend image
â”‚  â””â”€ Push frontend image
â”‚
â””â”€ Job 5: Deploy (requires build to pass)
   â”œâ”€ Setup kubectl
   â”œâ”€ Get kubeconfig from secrets
   â”œâ”€ kubectl apply -f k8s/
   â”œâ”€ Wait for rollout (5min timeout)
   â”œâ”€ Send Slack notification
   â””â”€ On failure: Automatic rollback
```

### Image Registry

```
GitHub Container Registry (GHCR)
â””â”€ ghcr.io/username/
   â”œâ”€ dhakacart-backend
   â”‚  â”œâ”€ main-latest
   â”‚  â”œâ”€ main-SHA12345
   â”‚  â””â”€ v1.0.0
   â”‚
   â””â”€ dhakacart-frontend
      â”œâ”€ main-latest
      â”œâ”€ main-SHA12345
      â””â”€ v1.0.0
```

---

## ğŸš€ Scaling Strategy

### Horizontal Pod Autoscaling (HPA)

**Backend HPA**
```
Target: Deployment/dhakacart-backend
Min Replicas: 3
Max Replicas: 10
Metrics:
  - CPU: 70% utilization
  - Memory: 80% utilization

Behavior:
  Scale Up: 
    - 100% increase every 15 seconds
    - +2 pods every 15 seconds
  Scale Down:
    - 50% decrease every 15 seconds (after 5min stable)
    - -1 pod every 15 seconds
```

**Load Profile**

```
Time          Traffic    Backend Pods   Frontend Pods
00:00-06:00   Low (1k)   3              3
06:00-09:00   Medium(15k) 5             4
09:00-18:00   High (50k) 8-10           6-8
18:00-23:00   Medium(25k) 6             5
23:00-00:00   Low (2k)   3              3

Peak Eid Sale: 100k+ users
â”œâ”€ Backend scales to 10 pods (max)
â”œâ”€ Frontend scales to 8 pods (max)
â””â”€ Load balancer distributes across all
```

---

## ğŸ“ˆ Capacity Planning

### Resource Allocation

**Per Backend Pod:**
- CPU: 500m (half core)
- Memory: 512Mi
- Network: ~10 Mbps (varies)
- Estimated users: 10,000 (with 3 replicas)

**Per Frontend Pod:**
- CPU: 250m
- Memory: 256Mi
- Network: ~5 Mbps
- Estimated requests: 5,000/sec (with 3 replicas)

**Total Cluster (Max Scaling):**
- Backend: 10 pods Ã— 500m = 5 vCPU
- Frontend: 8 pods Ã— 250m = 2 vCPU
- Monitoring: 1 vCPU
- **Total: ~8 vCPU equivalent**

---

## ğŸ”§ Technology Decisions & Rationale

### Why Kubernetes (K3s)?
- âœ… Auto-healing (restart failed pods)
- âœ… Auto-scaling (respond to load)
- âœ… Rolling updates (zero downtime)
- âœ… Multi-host deployment
- âœ… Built-in monitoring/logging hooks
- âœ… Industry standard

### Why Terraform?
- âœ… Infrastructure as Code (reproducible)
- âœ… Version control (git history)
- âœ… Modular (reusable components)
- âœ… AWS native support
- âœ… State management

### Why GitHub Actions?
- âœ… Native GitHub integration
- âœ… Free for public repos
- âœ… Extensive marketplace
- âœ… Easy configuration (YAML)
- âœ… Matrix testing support

### Why AWS RDS?
- âœ… Managed service (no ops overhead)
- âœ… Multi-AZ automatic failover
- âœ… Automated backups
- âœ… Encryption at rest
- âœ… CloudWatch integration

---

## ğŸ“Š Performance Targets

| Metric | Target | How Achieved |
|--------|--------|--------------|
| Response Time (p95) | < 200ms | Optimized Node.js, caching |
| Availability | 99.9% | 3+ replicas, failover |
| Auto-scale Time | < 1 min | HPA checks every 15s |
| Deployment Time | < 10 min | Parallel builds, push |
| MTTD (Mean Time To Detect) | < 1 min | Prometheus alerts |
| MTTR (Mean Time To Recover) | < 1 min | Auto-failover + probes |

---

**Architecture Version:** 1.0  
**Last Updated:** December 11, 2025
